from bs4 import BeautifulSoup
import requests
import os.path
import json
import io
from selenium import webdriver      
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

company = 'Adidas'
htmlFile = 'temp/' + company + " html.txt"
baseurl = "https://careers.adidas-group.com"
url = baseurl + "/jobs/"

# Note you must download chromedriver.exe and have webdriver.Chrome navigate to it for this to work correctly
driver = webdriver.Chrome('C:\\Users\\jarno\\OneDrive\\Documents\\MyStuff\\LiveProject2\\chromedriver.exe')
driver.get(url)

# Filter by IT positions
filterKeyword = driver.find_element_by_id("filterKeyword")
filterKeyword.send_keys('information technology')
time.sleep(2)


# This will essentially click the "Load More" button until there are no more jobs to load
while True:
    try:
        loadMoreButton = driver.find_element_by_id("search-result__load-more")
        loadMoreButton.send_keys('\n')
    except Exception as e:
        break

time.sleep(2)
html_page = driver.page_source
driver.quit()


# Check if the text copy of the webpage exists.  If not, create a text copy of it to parse through
if os.path.exists(htmlFile):
    # read the raw data
    with io.open(htmlFile, 'r', encoding='utf8') as file:
        data = file.read()

else:
    # access the website
    r  = requests.get(url)
    try:
        r.raise_for_status()
    except Exception as exc:
        print('There was a problem: %s' % (exc))

    data = r.text

    # Write the raw data
    os.makedirs(os.path.dirname(htmlFile), exist_ok=True)
    with io.open(htmlFile, 'w', encoding='utf8') as file:
        file.write(data)



# Used to pull out specific chunks of a string
def find_between( s, first, last ):
    try:
        start = s.index( first ) + len( first )
        end = s.index( last, start )
        return s[start:end]
    except ValueError:
        return ""



# Make the soup
soup = BeautifulSoup(html_page, "html.parser")

jobs = []

for link in soup.find_all('li', class_ = "job-list__job"):

    # Find the Application Link
    applicationLink = link.find_next('a').get('href')
    applicationLink = baseurl + applicationLink

    # Find the Job Title
    jobTitle = link.find_next('h3', class_ = "job-list__title").contents[0]

    # Find the Job Number and Date and split them up
    jobNumAndDate = link.find_next('span', class_ = "number-and-date").contents[0]
    jobNum = find_between(jobNumAndDate,"        ","-").strip()
    datePosted = find_between(jobNumAndDate, "-", "\n")

    # Find the location and break it down into the correct format
    jobFacts = link.find_next('p', class_ = "job-list__facts").contents[0]
    city = jobFacts.split("/",1)[0].strip()
    state = jobFacts.split("/",2)[1].strip()
    location = city + ", " + state

    # Only take jobs that are in Oregon
    if state == "OR":
        # Build an individual job
        job = {'ApplicationLink': applicationLink,
               'Company': company,
               'DatePosted': datePosted,
               'Experience': '',
               'Hours': '',
               'JobID': jobNum,
               'JobTitle': jobTitle,
               'LanguagesUsed' : '',
               'Location' : location,
               'Salary' : '',
        }

        # put all the jobs into an array, so JSON dumps correctly
        # print(sorted(job.items()))
        jobs.append(job)



# Print the json
with open(company + '.json', 'w') as outfile:
     json.dump(jobs, outfile)



        
